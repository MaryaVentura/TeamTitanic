{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msadmin/courses/deeplearning1/nbs/data/iceberg\n"
     ]
    }
   ],
   "source": [
    "%cd  /home/msadmin/courses/deeplearning1/nbs/data/iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create references to important directories we will use over and over\n",
    "import os, sys\n",
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir + '/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/msadmin/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "#Allow relative imports to directories above lesson1/\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "#import modules\n",
    "from utils import *\n",
    "from vgg16 import Vgg16\n",
    "\n",
    "#Instantiate plotting tool\n",
    "#In Jupyter notebooks, you will need to run this command before doing any plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create directories\n",
    "%cd $DATA_HOME_DIR\n",
    "%mkdir -p valid\n",
    "%mkdir -p results\n",
    "%mkdir -p train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide iceberg/ship images into separate directories\n",
    "\n",
    "%cd $DATA_HOME_DIR/valid\n",
    "%mkdir -p iceberg\n",
    "%mkdir -p ship\n",
    "\n",
    "%cd $DATA_HOME_DIR/train\n",
    "%mkdir -p iceberg\n",
    "%mkdir -p ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msadmin/courses/deeplearning1/nbs/data/iceberg/data/processed\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR/processed\n",
    "train_df = pd.read_json(\"train.json\")\n",
    "test_df = pd.read_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msadmin/courses/deeplearning1/nbs/data/iceberg/data/train\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33696"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_jpg():\n",
    "    import cv2\n",
    "\n",
    "    # fig = plt.gcf()\n",
    "    print(\"train_df.size\",len(train_df))\n",
    "    for ix, row in train_df.iterrows():\n",
    "        img = np.array(row['band_1']).reshape((75, 75))\n",
    "\n",
    "        img2 = np.array(row['band_2']).reshape((75, 75))\n",
    "        img3 = img + img2\n",
    "        img3 -= img3.min()\n",
    "        img3 /= img3.max()\n",
    "        img3 *= 255\n",
    "        #plt.imshow(img3)\n",
    "        img3 = img3.astype(np.uint8)\n",
    "        if row['is_iceberg']==0:\n",
    "\n",
    "            cv2.imwrite(\"ship/f{}.png\".format(ix), img3)\n",
    "            # fig.savefig(\"../input/train/0/f{}.png\".format(ix), dpi=100)\n",
    "        elif row['is_iceberg']==1:\n",
    "            cv2.imwrite(\"iceberg/f{}.png\".format(ix), img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train_df.size', 1604)\n"
     ]
    }
   ],
   "source": [
    "convert_to_jpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msadmin/courses/deeplearning1/nbs/data/iceberg/data/processed\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR/processed\n",
    "test_df = pd.read_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_test_to_png():\n",
    "    import cv2\n",
    "\n",
    "    # fig = plt.gcf()\n",
    "    print(\"test_df.size\",len(test_df))\n",
    "    for ix, row in test_df.iterrows():\n",
    "        img = np.array(row['band_1']).reshape((75, 75))\n",
    "\n",
    "        img2 = np.array(row['band_2']).reshape((75, 75))\n",
    "        img3 = img + img2\n",
    "        img3 -= img3.min()\n",
    "        img3 /= img3.max()\n",
    "        img3 *= 255\n",
    "        plt.imshow(img3)\n",
    "        img3 = img3.astype(np.uint8)\n",
    "        img_name = row['id']\n",
    "        cv2.imwrite(\"unknown/\"+ img_name + \".png\" , img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_single_to_png():\n",
    "    import cv2\n",
    "\n",
    "    # fig = plt.gcf()\n",
    "    #print(\"test_df.size\",len(test_df))\n",
    "    for ix, row in test_df[0:1].iterrows():\n",
    "        img = np.array(row['band_1']).reshape((75, 75))\n",
    "\n",
    "        img2 = np.array(row['band_2']).reshape((75, 75))\n",
    "        img3 = img + img2\n",
    "        img3 -= img3.min()\n",
    "        img3 /= img3.max()\n",
    "        img3 *= 255\n",
    "        \n",
    "        img3 = img3.astype(np.uint8)\n",
    "        img3 = ndimage.gaussian_filter(img3, sigma=3)\n",
    "        plt.imshow(img3)\n",
    "        img_name = row['id']\n",
    "        #cv2.imwrite(\"unknown/\"+ img_name + \".png\" , img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8cHvOD3FB7/j9JT/D6TQsIUQbOF8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ef05c5610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test_df[0:1]\n",
    "convert_single_to_png()\n",
    "#%mv *.png unknown/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?ndimage.gaussian_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative to converting to png -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "X_band_3=(X_band_1+X_band_2)/2\n",
    "#X_band_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in train[\"inc_angle\"]])\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis]\n",
    "                          , X_band_2[:, :, :, np.newaxis]\n",
    "                         , X_band_3[:, :, :, np.newaxis]], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_band_test_3=(X_band_test_1+X_band_test_2)/2\n",
    "#X_band_test_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in test[\"inc_angle\"]])\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n",
    "                          , X_band_test_2[:, :, :, np.newaxis]\n",
    "                         , X_band_test_3[:, :, :, np.newaxis]], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR/train/iceberg\n",
    "g = glob('*.png')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(len(g)/3): os.rename(shuf[i], DATA_HOME_DIR+'/valid/iceberg/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR/train/ship\n",
    "g = glob('*.png')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(len(g)/3): os.rename(shuf[i], DATA_HOME_DIR+'/valid/ship/' + shuf[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 714 images belonging to 2 classes.\n",
      "Found 356 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(DATA_HOME_DIR+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(DATA_HOME_DIR+'valid', batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.preprocessing.image.DirectoryIterator"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 714 images belonging to 2 classes.\n",
      "Found 356 images belonging to 2 classes.\n",
      "Found 8424 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(DATA_HOME_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches, ep, lr):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.optimizer.lr = lr\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=ep, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "714/714 [==============================] - 16s - loss: 1.4224 - acc: 0.6092 - val_loss: 4.6450 - val_acc: 0.5197\n",
      "Epoch 2/5\n",
      "714/714 [==============================] - 15s - loss: 0.5062 - acc: 0.7577 - val_loss: 5.6145 - val_acc: 0.4410\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - 16s - loss: 0.3990 - acc: 0.8067 - val_loss: 4.4337 - val_acc: 0.4691\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - 16s - loss: 0.3360 - acc: 0.8389 - val_loss: 4.7505 - val_acc: 0.4719\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - 16s - loss: 0.3049 - acc: 0.8571 - val_loss: 3.9861 - val_acc: 0.4775\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches, 5, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "714/714 [==============================] - 16s - loss: 1.1794 - acc: 0.7171 - val_loss: 8.2849 - val_acc: 0.4691\n",
      "Epoch 2/5\n",
      "714/714 [==============================] - 16s - loss: 0.4177 - acc: 0.8235 - val_loss: 1.1919 - val_acc: 0.4663\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - 16s - loss: 0.3182 - acc: 0.8641 - val_loss: 1.7427 - val_acc: 0.4691\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - 15s - loss: 0.2147 - acc: 0.9272 - val_loss: 1.0923 - val_acc: 0.5365\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - 16s - loss: 0.1539 - acc: 0.9552 - val_loss: 0.6296 - val_acc: 0.7275\n"
     ]
    }
   ],
   "source": [
    "#cnmem=.75\n",
    "model = conv1(batches, 5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "714/714 [==============================] - 17s - loss: 0.7464 - acc: 0.7395 - val_loss: 2.9751 - val_acc: 0.4691\n",
      "Epoch 2/5\n",
      "714/714 [==============================] - 15s - loss: 0.3721 - acc: 0.8515 - val_loss: 0.7921 - val_acc: 0.5365\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - 16s - loss: 0.1929 - acc: 0.9426 - val_loss: 0.8997 - val_acc: 0.5393\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - 16s - loss: 0.1167 - acc: 0.9818 - val_loss: 0.9385 - val_acc: 0.5309\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - 16s - loss: 0.0711 - acc: 0.9916 - val_loss: 1.3817 - val_acc: 0.5309\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches, 5, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "714/714 [==============================] - 17s - loss: 0.7367 - acc: 0.6821 - val_loss: 0.7350 - val_acc: 0.5140\n",
      "Epoch 2/10\n",
      "714/714 [==============================] - 15s - loss: 0.5066 - acc: 0.7689 - val_loss: 0.6735 - val_acc: 0.5730\n",
      "Epoch 3/10\n",
      "714/714 [==============================] - 16s - loss: 0.3403 - acc: 0.8403 - val_loss: 0.6728 - val_acc: 0.5702\n",
      "Epoch 4/10\n",
      "714/714 [==============================] - 16s - loss: 0.2536 - acc: 0.9006 - val_loss: 0.7166 - val_acc: 0.5337\n",
      "Epoch 5/10\n",
      "714/714 [==============================] - 16s - loss: 0.1989 - acc: 0.9468 - val_loss: 0.6582 - val_acc: 0.5787\n",
      "Epoch 6/10\n",
      "714/714 [==============================] - 17s - loss: 0.1846 - acc: 0.9370 - val_loss: 0.7628 - val_acc: 0.5309\n",
      "Epoch 7/10\n",
      "714/714 [==============================] - 17s - loss: 0.1277 - acc: 0.9832 - val_loss: 0.7112 - val_acc: 0.5365\n",
      "Epoch 8/10\n",
      "714/714 [==============================] - 16s - loss: 0.1035 - acc: 0.9902 - val_loss: 0.6859 - val_acc: 0.5393\n",
      "Epoch 9/10\n",
      "714/714 [==============================] - 16s - loss: 0.0890 - acc: 0.9916 - val_loss: 0.7082 - val_acc: 0.5449\n",
      "Epoch 10/10\n",
      "714/714 [==============================] - 16s - loss: 0.0802 - acc: 0.9930 - val_loss: 0.6804 - val_acc: 0.5590\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches, 10, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "714/714 [==============================] - 16s - loss: 0.7739 - acc: 0.6415 - val_loss: 1.9265 - val_acc: 0.5309\n",
      "Epoch 2/5\n",
      "714/714 [==============================] - 16s - loss: 0.7758 - acc: 0.6471 - val_loss: 1.4214 - val_acc: 0.5281\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - 16s - loss: 0.6558 - acc: 0.6905 - val_loss: 1.2920 - val_acc: 0.5281\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - 16s - loss: 0.5884 - acc: 0.7129 - val_loss: 1.1416 - val_acc: 0.5197\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - 16s - loss: 0.6144 - acc: 0.7129 - val_loss: 1.0591 - val_acc: 0.5197\n"
     ]
    }
   ],
   "source": [
    "model = conv1(batches, 5, 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_17 (BatchNorm (None, 3, 224, 224)   12          batchnormalization_input_5[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 32, 222, 222)  896         batchnormalization_17[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_18 (BatchNorm (None, 32, 222, 222)  128         convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_9 (MaxPooling2D)    (None, 32, 74, 74)    0           batchnormalization_18[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 64, 72, 72)    18496       maxpooling2d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_19 (BatchNorm (None, 64, 72, 72)    256         convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 64, 24, 24)    0           batchnormalization_19[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 36864)         0           maxpooling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 200)           7373000     flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_20 (BatchNorm (None, 200)           800         dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 2)             402         batchnormalization_20[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 7,393,990\n",
      "Trainable params: 7,393,392\n",
      "Non-trainable params: 598\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?image.ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(DATA_HOME_DIR+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?image.ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 714 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "714/714 [==============================] - 25s - loss: 0.6106 - acc: 0.7745 - val_loss: 0.7464 - val_acc: 0.6039\n",
      "Epoch 2/10\n",
      "714/714 [==============================] - 25s - loss: 0.6092 - acc: 0.7773 - val_loss: 0.7544 - val_acc: 0.6236\n",
      "Epoch 3/10\n",
      "714/714 [==============================] - 26s - loss: 0.6140 - acc: 0.7647 - val_loss: 0.7691 - val_acc: 0.6124\n",
      "Epoch 4/10\n",
      "714/714 [==============================] - 25s - loss: 0.6587 - acc: 0.7661 - val_loss: 0.7114 - val_acc: 0.6685\n",
      "Epoch 5/10\n",
      "714/714 [==============================] - 26s - loss: 0.6066 - acc: 0.7717 - val_loss: 0.6766 - val_acc: 0.6742\n",
      "Epoch 6/10\n",
      "714/714 [==============================] - 26s - loss: 0.5535 - acc: 0.7689 - val_loss: 0.6374 - val_acc: 0.6938\n",
      "Epoch 7/10\n",
      "714/714 [==============================] - 26s - loss: 0.5812 - acc: 0.7815 - val_loss: 0.6181 - val_acc: 0.7107\n",
      "Epoch 8/10\n",
      "714/714 [==============================] - 26s - loss: 0.5366 - acc: 0.8011 - val_loss: 0.6404 - val_acc: 0.6938\n",
      "Epoch 9/10\n",
      "714/714 [==============================] - 26s - loss: 0.5000 - acc: 0.7941 - val_loss: 0.6380 - val_acc: 0.6798\n",
      "Epoch 10/10\n",
      "714/714 [==============================] - 26s - loss: 0.5581 - acc: 0.7773 - val_loss: 0.6446 - val_acc: 0.6966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f0dc120d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15)\n",
    "model.optimizer.lr=0.00001\n",
    "batches = get_batches(DATA_HOME_DIR+'train', gen_t, batch_size=batch_size)\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 714 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "714/714 [==============================] - 26s - loss: 0.4202 - acc: 0.8347 - val_loss: 0.5934 - val_acc: 0.7135\n",
      "Epoch 2/5\n",
      "714/714 [==============================] - 26s - loss: 0.4365 - acc: 0.8459 - val_loss: 0.5942 - val_acc: 0.7275\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - 25s - loss: 0.4133 - acc: 0.8403 - val_loss: 0.5998 - val_acc: 0.7275\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - 26s - loss: 0.4366 - acc: 0.8347 - val_loss: 0.5673 - val_acc: 0.7303\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - 26s - loss: 0.4557 - acc: 0.8333 - val_loss: 0.5450 - val_acc: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f018f14d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=5)\n",
    "model.optimizer.lr=0.00001\n",
    "batches = get_batches(DATA_HOME_DIR+'train', gen_t, batch_size=batch_size)\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 714 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "714/714 [==============================] - 26s - loss: 0.4926 - acc: 0.8095 - val_loss: 0.5251 - val_acc: 0.7612\n",
      "Epoch 2/5\n",
      "714/714 [==============================] - 25s - loss: 0.3951 - acc: 0.8571 - val_loss: 0.5050 - val_acc: 0.7781\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - 25s - loss: 0.4532 - acc: 0.8235 - val_loss: 0.4841 - val_acc: 0.7725\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - 26s - loss: 0.3752 - acc: 0.8445 - val_loss: 0.4730 - val_acc: 0.7893\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - 26s - loss: 0.4341 - acc: 0.8347 - val_loss: 0.4798 - val_acc: 0.7893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f018f1390>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=10)\n",
    "model.optimizer.lr=0.00001\n",
    "batches = get_batches(DATA_HOME_DIR+'train', gen_t, batch_size=batch_size)\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 714 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "714/714 [==============================] - 26s - loss: 0.5543 - acc: 0.7815 - val_loss: 0.4400 - val_acc: 0.7949\n",
      "Epoch 2/5\n",
      "714/714 [==============================] - 26s - loss: 0.5506 - acc: 0.7871 - val_loss: 0.4282 - val_acc: 0.8062\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - 26s - loss: 0.5445 - acc: 0.7885 - val_loss: 0.4301 - val_acc: 0.8090\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - 25s - loss: 0.4526 - acc: 0.8249 - val_loss: 0.4618 - val_acc: 0.7781\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - 26s - loss: 0.4598 - acc: 0.8305 - val_loss: 0.4455 - val_acc: 0.8118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f0dc34550>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=25)\n",
    "model.optimizer.lr=0.00001\n",
    "batches = get_batches(DATA_HOME_DIR+'train', gen_t, batch_size=batch_size)\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 714 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "714/714 [==============================] - 27s - loss: 0.4906 - acc: 0.8123 - val_loss: 0.4171 - val_acc: 0.8006\n",
      "Epoch 2/5\n",
      "714/714 [==============================] - 25s - loss: 0.5953 - acc: 0.7829 - val_loss: 0.4059 - val_acc: 0.8202\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - 26s - loss: 0.5134 - acc: 0.7983 - val_loss: 0.4025 - val_acc: 0.8090\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - 26s - loss: 0.5931 - acc: 0.7703 - val_loss: 0.4692 - val_acc: 0.8090\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - 26s - loss: 0.5119 - acc: 0.7997 - val_loss: 0.4894 - val_acc: 0.8062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f018f1590>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=30)\n",
    "model.optimizer.lr=0.00001\n",
    "batches = get_batches(DATA_HOME_DIR+'train', gen_t, batch_size=batch_size)\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 714 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "714/714 [==============================] - 26s - loss: 0.5921 - acc: 0.7759 - val_loss: 0.4356 - val_acc: 0.8090\n",
      "Epoch 2/5\n",
      "714/714 [==============================] - 26s - loss: 0.5242 - acc: 0.8053 - val_loss: 0.4187 - val_acc: 0.8062\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - 25s - loss: 0.5430 - acc: 0.7731 - val_loss: 0.4303 - val_acc: 0.7978\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - 26s - loss: 0.4930 - acc: 0.7955 - val_loss: 0.4453 - val_acc: 0.7949\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - 26s - loss: 0.5663 - acc: 0.7829 - val_loss: 0.4298 - val_acc: 0.7921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f0de427d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=40)\n",
    "model.optimizer.lr=0.00001\n",
    "batches = get_batches(DATA_HOME_DIR+'train', gen_t, batch_size=batch_size)\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 714 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "714/714 [==============================] - 26s - loss: 0.4357 - acc: 0.8263 - val_loss: 0.4204 - val_acc: 0.8034\n",
      "Epoch 2/5\n",
      "714/714 [==============================] - 26s - loss: 0.4387 - acc: 0.8207 - val_loss: 0.4083 - val_acc: 0.8174\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - 26s - loss: 0.3531 - acc: 0.8599 - val_loss: 0.4177 - val_acc: 0.8258\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - 25s - loss: 0.4123 - acc: 0.8333 - val_loss: 0.4061 - val_acc: 0.8062\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - 26s - loss: 0.3907 - acc: 0.8459 - val_loss: 0.3850 - val_acc: 0.8230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f1891df90>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(height_shift_range=0.05)\n",
    "model.optimizer.lr=0.00001\n",
    "batches = get_batches(DATA_HOME_DIR+'train', gen_t, batch_size=batch_size)\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=10, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches, 5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=15, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four conv/pooling pairs + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(DATA_HOME_DIR+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Convolution2D(32,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(64,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Convolution2D(128,3,3, activation='relu'),\n",
    "        BatchNormalization(axis=1),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(200, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=10e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "714/714 [==============================] - 26s - loss: 0.1938 - acc: 0.9188 - val_loss: 0.6796 - val_acc: 0.6067\n",
      "Epoch 2/5\n",
      "714/714 [==============================] - 26s - loss: 0.2443 - acc: 0.9020 - val_loss: 0.6693 - val_acc: 0.6236\n",
      "Epoch 3/5\n",
      "714/714 [==============================] - 26s - loss: 0.2471 - acc: 0.9020 - val_loss: 0.7027 - val_acc: 0.6011\n",
      "Epoch 4/5\n",
      "714/714 [==============================] - 26s - loss: 0.1851 - acc: 0.9314 - val_loss: 0.7346 - val_acc: 0.5899\n",
      "Epoch 5/5\n",
      "714/714 [==============================] - 25s - loss: 0.1977 - acc: 0.9244 - val_loss: 0.7392 - val_acc: 0.5927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f02624bd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "batchnormalization_25 (BatchNorm (None, 3, 224, 224)   12          batchnormalization_input_7[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 32, 222, 222)  896         batchnormalization_25[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_26 (BatchNorm (None, 32, 222, 222)  128         convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_13 (MaxPooling2D)   (None, 32, 111, 111)  0           batchnormalization_26[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 64, 109, 109)  18496       maxpooling2d_13[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_27 (BatchNorm (None, 64, 109, 109)  256         convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_14 (MaxPooling2D)   (None, 64, 54, 54)    0           batchnormalization_27[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (None, 128, 52, 52)   73856       maxpooling2d_14[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_28 (BatchNorm (None, 128, 52, 52)   512         convolution2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_15 (MaxPooling2D)   (None, 128, 26, 26)   0           batchnormalization_28[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 86528)         0           maxpooling2d_15[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 200)           17305800    flatten_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_29 (BatchNorm (None, 200)           800         dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 200)           0           batchnormalization_29[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 200)           40200       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_30 (BatchNorm (None, 200)           800         dense_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 200)           0           batchnormalization_30[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 2)             402         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 17,442,158\n",
      "Trainable params: 17,440,904\n",
      "Non-trainable params: 1,254\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "714/714 [==============================] - 26s - loss: 1.0764 - acc: 0.6204 - val_loss: 0.9577 - val_acc: 0.4691\n",
      "Epoch 2/10\n",
      "714/714 [==============================] - 25s - loss: 0.8796 - acc: 0.6723 - val_loss: 0.7246 - val_acc: 0.4691\n",
      "Epoch 3/10\n",
      "714/714 [==============================] - 26s - loss: 0.6391 - acc: 0.7353 - val_loss: 0.6819 - val_acc: 0.5562\n",
      "Epoch 4/10\n",
      "714/714 [==============================] - 25s - loss: 0.5948 - acc: 0.7507 - val_loss: 0.6343 - val_acc: 0.7051\n",
      "Epoch 5/10\n",
      "714/714 [==============================] - 26s - loss: 0.5499 - acc: 0.7689 - val_loss: 0.6490 - val_acc: 0.5927\n",
      "Epoch 6/10\n",
      "714/714 [==============================] - 26s - loss: 0.4549 - acc: 0.7983 - val_loss: 0.6855 - val_acc: 0.5309\n",
      "Epoch 7/10\n",
      "714/714 [==============================] - 26s - loss: 0.5083 - acc: 0.8011 - val_loss: 0.6723 - val_acc: 0.5253\n",
      "Epoch 8/10\n",
      "714/714 [==============================] - 25s - loss: 0.4341 - acc: 0.8179 - val_loss: 0.6807 - val_acc: 0.5309\n",
      "Epoch 9/10\n",
      "714/714 [==============================] - 26s - loss: 0.4010 - acc: 0.8333 - val_loss: 0.6481 - val_acc: 0.5393\n",
      "Epoch 10/10\n",
      "714/714 [==============================] - 26s - loss: 0.3256 - acc: 0.8697 - val_loss: 0.6484 - val_acc: 0.5478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f2c8b1910>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=10, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr 0.001 looks to be too low.  Try a higher value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=10, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=10, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.00001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it bombed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0005\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try non augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "examine score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "#Helper function to plot images by index in the validation set \n",
    "#Plots is a helper function in utils.py\n",
    "def plots_idx(idx, titles=None):\n",
    "    plots([image.load_img(valid_path + filenames[i]) for i in idx], titles=titles)\n",
    "    \n",
    "#Number of images to view for each visualization task\n",
    "n_view = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_path=DATA_HOME_DIR + '/valid/'\n",
    "test_batches = get_batches(valid_path, shuffle=False, batch_size=batch_size, class_mode=None)\n",
    "val_batches, probs =  test_batches, model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = val_batches.filenames\n",
    "expected_labels = val_batches.classes #0 or 1\n",
    "\n",
    "#Round our predictions to 0/1 to generate labels\n",
    "our_predictions = probs[:,0]\n",
    "our_labels = np.round(1-our_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1. A few correct labels at random\n",
    "correct = np.where(our_labels==expected_labels)[0]\n",
    "print \"Found %d correct labels\" % len(correct)\n",
    "idx = permutation(correct)[:n_view]\n",
    "plots_idx(idx, our_predictions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. A few incorrect labels at random\n",
    "incorrect = np.where(our_labels!=expected_labels)[0]\n",
    "print \"Found %d incorrect labels\" % len(incorrect)\n",
    "idx = permutation(incorrect)[:n_view]\n",
    "plots_idx(idx, our_predictions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3a. The images we most confident were ships, and are actually ships\n",
    "correct_ships = np.where((our_labels==0) & (our_labels==expected_labels))[0]\n",
    "print \"Found %d confident correct ship labels\" % len(correct_ships)\n",
    "most_correct_ships = np.argsort(our_predictions[correct_ships])[::-1][:n_view]\n",
    "plots_idx(correct_ships[most_correct_ships], our_predictions[correct_ships][most_correct_ships])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3b. The images we most confident were dogs, and are actually dogs\n",
    "correct_iceberg = np.where((our_labels==1) & (our_labels==expected_labels))[0]\n",
    "print \"Found %d confident correct iceberg labels\" % len(correct_iceberg)\n",
    "most_correct_iceberg = np.argsort(our_predictions[correct_iceberg])[:n_view]\n",
    "plots_idx(correct_iceberg[most_correct_iceberg], our_predictions[correct_iceberg][most_correct_iceberg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4a. The images we were most confident were ships, but are actually icebergs\n",
    "incorrect_ships = np.where((our_labels==0) & (our_labels!=expected_labels))[0]\n",
    "print \"Found %d incorrect ships\" % len(incorrect_ships)\n",
    "if len(incorrect_ships):\n",
    "    most_incorrect_ships = np.argsort(our_predictions[incorrect_ships])[::-1][:n_view]\n",
    "    plots_idx(incorrect_ships[most_incorrect_ships], our_predictions[incorrect_ships][most_incorrect_ships])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4b. The images we were most confident were dogs, but are actually cats\n",
    "incorrect_iceberg = np.where((our_labels==1) & (our_labels!=expected_labels))[0]\n",
    "print \"Found %d incorrect icebergs\" % len(incorrect_iceberg)\n",
    "if len(incorrect_iceberg):\n",
    "    most_incorrect_iceberg = np.argsort(our_predictions[incorrect_iceberg])[:n_view]\n",
    "    plots_idx(incorrect_iceberg[most_incorrect_iceberg], our_predictions[incorrect_iceberg][most_incorrect_iceberg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(expected_labels, our_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize Log Loss when True value = 1\n",
    "#y-axis is log loss, x-axis is probabilty that label = 1\n",
    "#As you can see Log Loss increases rapidly as we approach 0\n",
    "#But increases slowly as our predicted probability gets closer to 1\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "x = [i*.0001 for i in range(1,10000)]\n",
    "y = [log_loss([1],[[i*.0001,1-(i*.0001)]],eps=1e-15) for i in range(1,10000,1)]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.axis([-.05, 1.1, -.8, 10])\n",
    "plt.title(\"Log Loss when true label = 1\")\n",
    "plt.xlabel(\"predicted probability\")\n",
    "plt.ylabel(\"log loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try shallower network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    BatchNormalization(axis=1, input_shape=(3, 224, 224)),\n",
    "    Convolution2D(32,3,3, activation='relu'),\n",
    "    BatchNormalization(axis=1),\n",
    "    MaxPooling2D((3,3)),\n",
    "    Convolution2D(64,3,3, activation='relu'),\n",
    "    BatchNormalization(axis=1),\n",
    "    MaxPooling2D((3,3)),\n",
    "    Flatten(),\n",
    "    Dense(200, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(Adam(lr=10e-5), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(DATA_HOME_DIR+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(DATA_HOME_DIR+'valid', batch_size=batch_size*2, shuffle=False)\n",
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=10, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(path, batch_size=8):\n",
    "    test_batches = get_batches(path, shuffle=False, batch_size=batch_size, class_mode=None)\n",
    "    return test_batches, model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8424 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_path = DATA_HOME_DIR + '/test/' #We use all the test data\n",
    "batches, preds = test(test_path, batch_size = batch_size*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(preds[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.0310e-03,   9.9397e-01],\n",
       "       [  2.4804e-04,   9.9975e-01]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Predictions: [  9.9397e-01   9.9975e-01   9.9944e-01   7.5562e-04   5.7040e-04]\n",
      "Mid Predictions: [ 0.562   0.5594  0.5219  0.5294  0.5364  0.5663  0.4744  0.5196  0.4692  0.4067  0.4637  0.4595\n",
      "  0.5891  0.5718  0.4151  0.484   0.5953  0.4148  0.4656  0.4448  0.5736  0.4382  0.5124  0.5722\n",
      "  0.4906  0.5401  0.4833  0.4232  0.44    0.5628  0.4801  0.5498  0.5017  0.5254  0.5916  0.4897\n",
      "  0.4402  0.5505  0.539   0.5741  0.4264  0.5308  0.4899  0.5041  0.582   0.5816  0.5456  0.4629\n",
      "  0.4762  0.5095  0.4378  0.5703  0.4399  0.5497  0.5119  0.4326  0.454   0.4097  0.5162  0.5644\n",
      "  0.4236  0.5849  0.5819  0.5384  0.5451  0.5432  0.4563  0.5517  0.5838  0.5932  0.567   0.4933\n",
      "  0.5008  0.4603  0.5793  0.5656  0.5551  0.4689  0.4072  0.4301  0.4878  0.4162  0.4622  0.4745\n",
      "  0.5453  0.4033  0.5798  0.4745  0.5333  0.4876  0.4269  0.4468  0.5801  0.4061  0.5845  0.5092\n",
      "  0.427   0.5797  0.4486  0.4315  0.4456  0.5498  0.4057  0.4016  0.581   0.4106  0.4415  0.4059\n",
      "  0.5365  0.5759  0.5021  0.4047  0.4986  0.4159  0.4534  0.4645  0.4599  0.4551  0.4149  0.536\n",
      "  0.469   0.5844  0.5772  0.5457  0.4344  0.412   0.5477  0.5543  0.5879  0.5812  0.521   0.4345\n",
      "  0.4807  0.5519  0.5255  0.5863  0.5263  0.4558  0.4448  0.5673  0.5622  0.445   0.4389  0.475\n",
      "  0.561   0.5829  0.546   0.4419  0.5822  0.5449  0.5024  0.4993  0.4787  0.4576  0.4237  0.5944\n",
      "  0.5623  0.455   0.5084  0.5099  0.4038  0.4369  0.5542  0.5559  0.4901  0.4862  0.4321  0.4831\n",
      "  0.5865  0.5779  0.5636  0.4145  0.5799  0.5931  0.4176  0.5585  0.4907  0.5582  0.4448  0.5925\n",
      "  0.4962  0.4469  0.5191  0.4284  0.5594  0.5758  0.494   0.5969  0.5652  0.5177  0.4453  0.4249\n",
      "  0.4906  0.5247  0.5527  0.4314  0.5055  0.5853  0.4049  0.5283  0.5712  0.5398  0.4476  0.5436\n",
      "  0.5283  0.5765  0.5387  0.5734  0.5814  0.5092  0.5329  0.5072  0.4718  0.456   0.4824  0.5202\n",
      "  0.5981  0.4954  0.5836  0.5035  0.561   0.4436  0.4194  0.4072  0.4363  0.5228  0.4783  0.5011\n",
      "  0.557   0.5296  0.4538  0.5633  0.4192  0.5358  0.5551  0.5603  0.4579  0.5338  0.4517  0.595\n",
      "  0.4025  0.5844  0.5039  0.4685  0.5277  0.409   0.4899  0.4545  0.5847  0.5125  0.5101  0.4544\n",
      "  0.4222  0.4252  0.5388  0.4969  0.5807  0.5189  0.5283  0.406   0.4289  0.5533  0.5281  0.4686\n",
      "  0.5853  0.464   0.4471  0.5961  0.4896  0.5902  0.4684  0.5978  0.5447  0.4591  0.4896  0.5718\n",
      "  0.529   0.4717  0.5487  0.4199  0.4098  0.574   0.5171  0.4037  0.5127  0.5235  0.5394  0.53\n",
      "  0.5552  0.4208  0.4731  0.5989  0.4279  0.5602  0.491   0.5111  0.4492  0.5363  0.4919  0.4412\n",
      "  0.5594  0.4319  0.5832  0.4154  0.433   0.5999  0.488   0.4958  0.4978  0.418   0.5732  0.4222\n",
      "  0.4129  0.4021  0.5412  0.4995  0.43    0.4835  0.4348  0.5332  0.5668  0.4774  0.5522  0.4077\n",
      "  0.5597  0.4783  0.475   0.5548  0.4009  0.411   0.4128  0.4982  0.4652  0.5386  0.4385  0.5318\n",
      "  0.4542  0.5134  0.5407  0.468   0.4923  0.482   0.5002  0.511   0.5594  0.5192  0.5912  0.4868\n",
      "  0.5905  0.4323  0.5517  0.5698  0.4964  0.407   0.4124  0.4659  0.4271  0.45    0.579   0.5788\n",
      "  0.5358  0.5149  0.4277  0.4378  0.4791  0.5264  0.5602  0.4311  0.5776  0.5793  0.5105  0.4207\n",
      "  0.4393  0.5806  0.4449  0.5488  0.4799  0.4852  0.5747  0.5753  0.5357  0.5083  0.5268  0.4744\n",
      "  0.4164  0.5007  0.581   0.5653  0.4323  0.4703  0.5967  0.4947  0.4904  0.4925  0.4626  0.5683\n",
      "  0.5164  0.5592  0.5249  0.5294  0.5094  0.5078  0.5492  0.4996  0.5055  0.537   0.4056  0.5174\n",
      "  0.5439  0.5222  0.5713  0.5538  0.5702  0.5396  0.4174  0.4472  0.4548  0.5681  0.5279  0.4473\n",
      "  0.5141  0.431   0.4953  0.5626  0.5322  0.4611  0.5369  0.4217  0.5993  0.5187  0.4784  0.479\n",
      "  0.4784  0.4606  0.528   0.5457  0.4491  0.4215  0.4069  0.4504  0.5914  0.4461  0.4622  0.499\n",
      "  0.5676  0.4024  0.4849  0.5164  0.431   0.4932  0.5179  0.5158  0.4792  0.4203  0.4991  0.5376\n",
      "  0.5498  0.4293  0.5943  0.5263  0.4679  0.5171  0.4247  0.4632  0.5332  0.411   0.4248  0.418\n",
      "  0.5008  0.5773  0.5691  0.4069  0.4088  0.4875  0.4726  0.5631  0.5143  0.5836  0.409   0.4467\n",
      "  0.4072  0.4618  0.4274  0.4327  0.4672  0.4898  0.591   0.5856  0.5653  0.5355  0.5429  0.4431\n",
      "  0.5666  0.4158  0.5194  0.5927  0.4457  0.5661  0.4198  0.4958  0.4088  0.5624  0.5348  0.4761\n",
      "  0.5277  0.5196  0.5931  0.5215  0.5912  0.5442  0.416   0.5432  0.4732  0.4609  0.5687  0.4401\n",
      "  0.5567  0.5099  0.4904  0.5394  0.5929  0.5369  0.5648  0.5956  0.4274  0.5835  0.4096  0.5192\n",
      "  0.4891  0.5805  0.4937  0.4883  0.4106  0.4333  0.4867  0.4714  0.4759  0.4442  0.41    0.5255\n",
      "  0.5418  0.4469  0.5394  0.5217  0.5158  0.586   0.5268  0.431   0.472   0.5476  0.5506  0.5201\n",
      "  0.5529  0.5876  0.5705  0.5986]\n",
      "Edge Predictions: []\n"
     ]
    }
   ],
   "source": [
    "#Grab the dog prediction column\n",
    "isiceberg = preds[:,1]\n",
    "print \"Raw Predictions: \" + str(isiceberg[:5])\n",
    "print \"Mid Predictions: \" + str(isiceberg[(isiceberg < .6) & (isiceberg > .4)])\n",
    "print \"Edge Predictions: \" + str(isiceberg[(isiceberg == 1) | (isiceberg == 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(isiceberg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#So to play it safe, we use a sneaky trick to round down our edge predictions\n",
    "#Swap all ones with .95 and all zeros with .05\n",
    "isiceberg = isiceberg.clip(min=0.15, max=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract imageIds from the filenames in our test/unknown directory \n",
    "filenames = batches.filenames\n",
    "#ids = np.array([int(f[8:f.find('.')]) for f in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f1188'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[0].split('/')[1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [f.split('/')[1].split('.')[0] for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-a9e27f17b6af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0misiceberg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/msadmin/anaconda2/lib/python2.7/site-packages/numpy/core/shape_base.pyc\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "subm = np.stack([ids,isiceberg], axis=1)\n",
    "subm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.string_"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(subm[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msadmin/courses/deeplearning1/nbs/data/iceberg/data/results\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR/results\n",
    "submission_file_name = 'submission1.csv'\n",
    "np.savetxt(submission_file_name, subm, fmt='%s,%s', header='id,is_iceberg', comments='')\n",
    "#np.savetxt(submission_file_name, subm, header='id,label', comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msadmin/courses/deeplearning1/nbs/data/iceberg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='data/results/submission1.csv' target='_blank'>data/results/submission1.csv</a><br>"
      ],
      "text/plain": [
       "/home/msadmin/courses/deeplearning1/nbs/data/iceberg/data/results/submission1.csv"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "%cd $LESSON_HOME_DIR\n",
    "FileLink('data/results/' + submission_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
